# Turn off the logs
# logging.level.root=OFF
# logging.level.org.springframework.boot=OFF
# spring.main.banner-mode=OFF
server.contextPath = /sqlproject
server.port = 8080
spring.jackson.date-format = yyyy-MM-dd HH:mm:ss
spring.jackson.time-zone = Asia/Shanghai

# 下面为连接池的补充设置，应用到下面所有数据源中
# 初始化大小，最小，最大
spring.datasource.initialSize=5
spring.datasource.minIdle=5
spring.datasource.maxActive=20
# 配置获取连接等待超时的时间
spring.datasource.maxWait=60000
# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
spring.datasource.timeBetweenEvictionRunsMillis=60000
# 配置一个连接在池中最小生存的时间，单位是毫秒
spring.datasource.minEvictableIdleTimeMillis=300000
# Oracle请使用select 1 from dual
spring.datasource.validationQuery=SELECT 'x'
spring.datasource.testWhileIdle=true
spring.datasource.testOnBorrow=false
spring.datasource.testOnReturn=false
# 打开PSCache，并且指定每个连接上PSCache的大小
spring.datasource.poolPreparedStatements=true
spring.datasource.maxPoolPreparedStatementPerConnectionSize=20
# 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
spring.datasource.filters=stat,wall,slf4j
# 通过connectProperties属性来打开mergeSql功能；慢SQL记录
spring.datasource.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
# 合并多个DruidDataSource的监控数据
spring.datasource.useGlobalDataSourceStat=true


###### oracle数据源 ######
# oracle.datasource.url=jdbc:oracle:thin:@193.5.111.50:1521/gyjg
# oracle.datasource.username=gyjg
# oracle.datasource.password=gyjg2018

#oracle.datasource.url=jdbc:oracle:thin:@10.0.91.175:1521/JAPRD
#oracle.datasource.username=CS_HAI2
#oracle.datasource.password=CS_HAI2

oracle.datasource.url=jdbc:oracle:thin:@192.168.1.170:1521:ICATI
oracle.datasource.username=cs_hai
oracle.datasource.password=ca_hai

oracle.datasource.driver-class-name=oracle.jdbc.OracleDriver
oracle.instance.type=sid


###### phoenix数据源 ######
# phoenix.enable = true
# phoenix.type=com.alibaba.druid.pool.DruidDataSource
# phoenix.default-auto-commit=true
# phoenix.schema.isNamespaceMappingEnabled=true

######################南京测试环境 10.0.91.95 phoenix jdbc配置########################
phoenix.datasource.url=jdbc:phoenix:10.0.91.95:2181:/hbase-unsecure
######################北京测试环境 192.168.1.74 phoenix jdbc配置######################
# phoenix.datasource.url=jdbc:phoenix:192.168.1.74:2181:/hbase-unsecure

phoenix.datasource.username=
phoenix.datasource.password=
phoenix.datasource.driver-class-name=org.apache.phoenix.jdbc.PhoenixDriver

##########################北京测试环境 192.168.1.74 nifi地址##########################
# nifi.address = http://192.168.1.74:9090/nifi-api

################南京测试环境 10.0.91.95 nifi.address配置################
nifi.address = http://10.0.91.95:9090/nifi-api

################南京测试环境 10.0.91.95 ambari.address配置################
ambari.address = http://10.0.91.95:8080/api/v1
cluster.name = hai

###########################南京测试环境 10.0.91.95 nifi配置###########################
nifi.mysql.driver.address = /usr/hdf/3.1.2.0-7/nifi/lib/mysql-connector-java-5.1.38.jar
nifi.oracle.driver.address = /usr/hdf/3.1.2.0-7/nifi/lib/ojdbc6-11.2.0.2.1.jar
nifi.phoenix.driver.address = /usr/hdp/2.6.5.0-292/phoenix/phoenix-4.7.0.2.6.5.0-292-client.jar
nifi.hdfs.config.address = /var/lib/nifi/conf/core-site.xml,/var/lib/nifi/conf/hdfs-site.xml
###########################北京测试环境 192.168.1.74 nifi配置#########################
# nifi.mysql.driver.address = /usr/hdf/3.3.1.0-10/nifi/lib/mysql-connector-java-5.1.38.jar
# nifi.oracle.driver.address = /usr/hdf/3.3.1.0-10/nifi/lib/ojdbc6-11.2.0.2.1.jar
# nifi.phoenix.driver.address = /usr/hdf/3.3.1.0-10/nifi/lib/phoenix-5.0.0.3.1.0.0-78-client.jar
# nifi.hdfs.config.address = /var/lib/nifi/conf/core-site.xml,/var/lib/nifi/conf/hdfs-site.xml

##############################南京测试环境 10.0.91.95配置#############################
hadoop.home.dir = /usr/hdp/2.6.5.0-292/hadoop
fs.default.path = hdfs://10.0.91.95:8020/test/
#############################北京测试环境 192.168.1.74配置############################
# hadoop.home.dir = /usr/hdp/3.1.0.0-78/hadoop
# fs.default.path = hdfs://192.168.1.74:8020



##############################南京测试环境 10.0.91.95配置#############################
dataflow.id = 1bcc989d-016b-1000-ffff-ffffef32bf8a
rules.id = 1bcc37c0-016b-1000-0000-00003697e5ec

#############################北京测试环境 192.168.1.74配置############################
#dataflow.id = c37247ea-016a-1000-ffff-ffffa260cbed
#rules.id = c371c8f8-016a-1000-ffff-ffffed9c2440


################################datamanagersystem整合#################################

spring.http.multipart.maxFileSize = 10MB
spring.http.multipart.maxRequestSize = 10MB

# spring.servlet.multipart.max-file-size = 1000Mb   这是springboot 2.0以后的表示
# spring.servlet.multipart.max-request-size = 1000Mb

mybatis.typeAliasesPackage = com.hiynn.gybigdata.mapper

logging.level.com.hiynn.gybigdata = debug

# 本地上传资料位置
folder: /home/webapp/hai/uploadfile
#folder: D://
# folder: /hai/uploadfile

# ftp信息和上传资料位置
ftp.host = 192.168.170.131
# ftp.host = 193.5.111.41
ftp.port = 21
ftp.username = ftp
ftp.password = ftp  
ftp.filepath = /pub

# 上传模式
file.mode = local
# file.mode = ftp

# 模板名称
template.ruledetail = ruledetailtemplate.xlsx
template.datasourcefield = datasourcefieldtemplate.xlsx
template.applicationregister = apimanual.docx

# zabbix配置
# zabbix.serverip = 192.168.170.131
# zabbix.port = 80
# zabbix.username = Admin
# zabbix.password = zabbix

spring.redis.host=10.0.91.93
spring.redis.password=123456
spring.redis.database=10
spring.redis.pool.max-active=8
spring.redis.pool.max-idle=8
spring.redis.pool.max-wait=-1
spring.redis.pool.min-idle=0
spring.redis.port=6339
# AES密码加密私钥(Base64加密)
encryptAESKey=V2FuZzkyNjQ1NGRTQkFQSUpXVA==
# JWT认证加密私钥(Base64加密)
encryptJWTKey=U0JBUElKV1RkV2FuZzkyNjQ1NA==
# AccessToken过期时间-2分钟-2*60(秒为单位)
accessTokenExpireTime=86400
# RefreshToken过期时间-7天-60(秒为单位)
refreshTokenExpireTime=604800
# Shiro缓存过期时间-2分钟-2*60(秒为单位)(与AccessToken过期时间一致)
shiroCacheExpireTime=86400

# Prometheus API
prometheus.apiPath=http://10.0.91.93:9090/api/v1/query
consul.apiPath=http://10.0.91.93:8500/v1/kv/
node_exporter.port=9100

checkruleresult.num = 20
spring.profiles.active=dev